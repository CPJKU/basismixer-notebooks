{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models\n",
    "\n",
    "In the last two notebooks we had a look at two of the components of the Basis Mixer. In this notebook we add the third part of the puzzle: the **Predictive Models**.\n",
    "\n",
    "A predictive model is defined as a mathematical which maps score information (encoded by the basis functions) $\\mathbf{\\Phi}$ to expressive parameters $\\mathbf{Y}$\n",
    "\n",
    "$$F(\\boldsymbol{\\Phi}) = \\mathbf{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from basismixer.predictive_models import (construct_model,\n",
    "                                          RecurrentModel,\n",
    "                                          SupervisedTrainer,\n",
    "                                          MSELoss)\n",
    "from basismixer.utils import load_pyc_bz, save_pyc_bz\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fn = ['../vienna4x22_notewise.pyc.bz', '../vienna4x22_onsetwise.pyc.bz']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the configuration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = [\n",
    "    dict(basis_functions='from_dataset',\n",
    "         parameter_names=['velocity_trend', 'timing', 'log_articulation'],\n",
    "         constructor=['basismixer.predictive_models', 'FeedForwardModel'],\n",
    "         args=dict(\n",
    "             hidden_size=128,\n",
    "             input_type='notewise'\n",
    "         ),\n",
    "         train_args=dict(\n",
    "             optimizer=dict(constructor='Adam',\n",
    "                            lr=1e-4),\n",
    "             epochs=3,\n",
    "             save_freq=1,\n",
    "             early_stopping=100,\n",
    "             batch_size=10,\n",
    "         )\n",
    "         ),\n",
    "    dict(basis_functions='from_dataset',\n",
    "         parameter_names=['velocity_trend', 'beat_period_standardized'],\n",
    "         constructor=['basismixer.predictive_models', 'RecurrentModel'],\n",
    "         args=dict(\n",
    "             recurrent_size=128,\n",
    "             n_layers=1,\n",
    "             hidden_size=64,\n",
    "             input_type='onsetwise'),\n",
    "         train_args=dict(\n",
    "             optimizer=dict(constructor='Adam',\n",
    "                            lr=1e-4),\n",
    "             epochs=3,\n",
    "             save_freq=1,\n",
    "             early_stopping=100,\n",
    "             batch_size=10,\n",
    "         )\n",
    "         )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the models\n",
    "\n",
    "Given a training set of expressive performances aligned to their scores, we can train the models in a supervised way by minimizing the *mean squared error* between predictions and the observed expressive parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloaders(dataset_fn, batch_size, valid_size=0.2):\n",
    "    # load dataset\n",
    "    data = load_pyc_bz(fn)\n",
    "    # Dataset\n",
    "    dataset = data['dataset']\n",
    "    # Input names (basis functions)\n",
    "    in_names = data['in_names']\n",
    "    # Output names (expressive parameters)\n",
    "    out_names = data['out_names']\n",
    "    \n",
    "    # Split dataset in training and validation\n",
    "    dataset_idx = np.arange(len(dataset))\n",
    "    rng.shuffle(dataset_idx)\n",
    "    len_valid = int(np.round(len(dataset) * valid_size))\n",
    "    valid_idx = dataset_idx[0:len_valid]\n",
    "    train_idx = dataset_idx[len_valid:]\n",
    "    \n",
    "    # Subset of the dataset for training\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    # Subset of the dataset for validation\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    LOGGER.info('Using {0} instances for')\n",
    "    train_dataloader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  # shuffle=True,\n",
    "                                  sampler=train_sampler)\n",
    "    valid_dataloader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1984)\n",
    "\n",
    "datasets = []\n",
    "models = []\n",
    "target_idxs = []\n",
    "input_idxs = []\n",
    "valid_size = 0.20\n",
    "for fn, config in zip(dataset_fn, model_config):\n",
    "\n",
    "    #### Construct Dataset ####\n",
    "    batch_size = config['train_args'].pop('batch_size')\n",
    "    # load dataset\n",
    "    data = load_pyc_bz(fn)\n",
    "    dataset = data['dataset']\n",
    "    in_names = data['in_names']\n",
    "    out_names = data['out_names']\n",
    "\n",
    "    dataset_idx = np.arange(len(dataset))\n",
    "    rng.shuffle(dataset_idx)\n",
    "    len_valid = int(np.round(len(dataset) * valid_size))\n",
    "    valid_idx = dataset_idx[0:len_valid]\n",
    "    train_idx = dataset_idx[len_valid:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    train_dataloader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  # shuffle=True,\n",
    "                                  sampler=train_sampler)\n",
    "    valid_dataloader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  sampler=valid_sampler)\n",
    "\n",
    "    datasets.append((dataset, train_dataloader, valid_dataloader))\n",
    "\n",
    "    #### Construct Models ####\n",
    "\n",
    "    if config['basis_functions'] == 'from_dataset':\n",
    "        model_in_names = in_names\n",
    "    elif isinstance(config['basis_functions'], (list, tuple)):\n",
    "        model_in_names = []\n",
    "        for bf in config['basis_functions']:\n",
    "            for name in in_names:\n",
    "                if name.startswith(bf):\n",
    "                    model_in_names.append(name)\n",
    "\n",
    "    i_idxs = np.array([list(in_names).index(bf) for bf in model_in_names])\n",
    "    input_idxs.append(i_idxs)\n",
    "\n",
    "    model_out_names = []\n",
    "    for pn in config['parameter_names']:\n",
    "        for name in out_names:\n",
    "            if name == pn:\n",
    "                model_out_names.append(name)\n",
    "\n",
    "    t_idxs = np.array([list(out_names).index(pn) for pn in model_out_names])\n",
    "    target_idxs.append(t_idxs)\n",
    "\n",
    "    config['args']['input_names'] = model_in_names\n",
    "    config['args']['input_size'] = len(model_in_names)\n",
    "\n",
    "    config['args']['output_names'] = model_out_names\n",
    "    config['args']['output_size'] = len(model_out_names)\n",
    "\n",
    "    model = construct_model(config)\n",
    "    models.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_loader, val_loader, out_dir):\n",
    "    # Get the configuration for the trainer\n",
    "    t_config = config['train_args']\n",
    "    # Get the configuration for the optimizer\n",
    "    o_config = t_config.pop('optimizer')\n",
    "    # Name of the model\n",
    "    model_name = '-'.join(model.output_names) + '-' + model.input_type\n",
    "    # Create a directory for storing the model parameters\n",
    "    model_out_dir = os.path.join(out_dir, model_name)\n",
    "    if not os.path.exists(model_out_dir):\n",
    "        os.mkdir(model_out_dir)\n",
    "    # Loss function\n",
    "    loss = MSELoss()\n",
    "    # Initialize the optimizer\n",
    "    optim = getattr(torch.optim, o_config.pop('constructor'))\n",
    "    optimizer = optim(model.parameters(), **o_config)\n",
    "    # Create trainer for training model in a supervised way\n",
    "    trainer = SupervisedTrainer(model=model,\n",
    "                                train_loss=loss,\n",
    "                                optimizer=optimizer,\n",
    "                                valid_loss=loss,\n",
    "                                train_dataloader=train_loader,\n",
    "                                valid_dataloader=val_loader,\n",
    "                                out_dir=model_out_dir,\n",
    "                                **t_config)\n",
    "    # train the mode\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model, dtv, config, t_idxs) in zip(models, datasets,\n",
    "                            model_config, target_idxs):\n",
    "    dataset, train_loader, val_loader = dtv\n",
    "    # set the indices of the desired targets in the dataset\n",
    "    for d in dataset.datasets:\n",
    "        d.targets_idx = t_idxs\n",
    "    # Train the models\n",
    "    train_model(model, config, train_loader, val_loader, out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
